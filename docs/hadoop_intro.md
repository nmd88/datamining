HADOOP INTRODUCTION
---

Apache HADOOP is a framework used to develop data processing applications which are executed in a distributed computing environment.

Similar to data residing in a local file system of personal computer system, in Hadoop, data resides in a distributed file system which is called as a Hadoop Distributed File system.

Processing model is based on 'Data Locality' concept wherein computational logic is sent to cluster nodes(server) containing data.

This computational logic is nothing but a compiled version of a program written in a high level language such as Java. 
Such a program, processes data stored in Hadoop HDFS.

HADOOP is an open source software framework. 
Applications built using HADOOP are run on large data sets distributed across clusters of commodity computers.

Commodity computers are cheap and widely available. 
These are mainly useful for achieving greater computational power at low cost. 

# 1. Components of Hadoop
## 1.1. Zookeeper - Coordination

## 1.2. Ambari - 
